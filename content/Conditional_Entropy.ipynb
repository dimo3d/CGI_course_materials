{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Bedingte Entropie\n## Zusammenhang zwischen Wetter und Kleidungswahl\n\nIn diesem Abschnitt untersuchen wir, wie Informationen über das Wetter unsere Wahl der Kleidung beeinflussen können. Wir werden Konzepte wie Entropie, bedingte Entropie, gemeinsame Entropie und gegenseitige Information anhand eines einfachen Beispiels veranschaulichen.\n\n## Beispielbeschreibung\n\nWir betrachten eine Situation, in der das Wetter (`X`) entweder \"Sonnig\" oder \"Regnerisch\" sein kann und die Wahl der Kleidung (`Y`) entweder \"Mantel\" oder \"T-Shirt\" umfasst. Die Wahrscheinlichkeiten für das Wetter sind gegeben, ebenso wie die bedingten Wahrscheinlichkeiten für die Wahl der Kleidung basierend auf dem Wetter. \n\nDas Ziel ist es, zu verstehen, wie viel Unsicherheit (oder Information) in der Wahl der Kleidung steckt, wenn wir das Wetter nicht kennen (Entropie von `Y`), wie viel Unsicherheit entfernt wird, wenn wir das Wetter kennen (bedingte Entropie von `Y` gegeben `X`), und wie viel Information das Wetter und die Kleidungswahl teilen (gegenseitige Information zwischen `X` und `Y`).\n\n## Lernziele\n\n- **Entropie `H(X)`**: Wie unvorhersehbar ist das Wetter?\n- **Entropie `H(Y)`**: Wie unvorhersehbar ist die Wahl der Kleidung ohne Kenntnis des Wetters?\n- **Gemeinsame Entropie `H(XY)`**: Wie unvorhersehbar sind sowohl das Wetter als auch die Wahl der Kleidung zusammen?\n- **Bedingte Entropie `H(Y|X)`**: Wie unvorhersehbar ist die Wahl der Kleidung, wenn das Wetter bekannt ist?\n- **Bedingte Entropie `H(X|Y)`**: Wie unvorhersehbar ist das Wetter, wenn die Kleidung bekannt ist?\n- **Synentropie, Transinformation `I(X;Y)`**: Wie viel reduziert das Wissen über das Wetter die Unsicherheit bei der Wahl der Kleidung?\n\nWir werden diese Metriken berechnen und interpretieren, um einen tieferen Einblick in die Beziehung zwischen Wetter und Kleidung zu gewinnen.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Formeln zur Berechnung der Entropie-Maße\n\nIn diesem Abschnitt verwenden wir folgende Formeln, um die verschiedenen Entropie-Maße für unser Beispiel zu berechnen:\n\n## Entropie von Wetter $H(X)$\nDie Entropie des Wetters wird berechnet als:\n$$ H(X) = -\\sum_{x \\in X} P(x) \\log_2 P(x) $$\n\n## Entropie von Kleidung $H(Y)$\nDie Entropie der Kleidung wird berechnet als:\n$$ H(Y) = -\\sum_{y \\in Y} P(y) \\log_2 P(y) $$\n\n## Gemeinsame Entropie $H(X, Y)$\nDie gemeinsame Entropie von Wetter und Kleidung wird berechnet als:\n$$ H(X, Y) = -\\sum_{x \\in X} \\sum_{y \\in Y} P(x, y) \\log_2 P(x, y) $$\n\n## Bedingte Entropie $H(Y|X)$\nDie bedingte Entropie der Kleidung gegeben das Wetter wird berechnet als:\n$$ H(Y|X) = -\\sum_{x \\in X} P(x) \\sum_{y \\in Y} P(y|x) \\log_2 P(y|x) $$\n\n## Bedingte Entropie $H(X|Y)$\nDie bedingte Entropie des Wetters gegeben die Kleidung wird berechnet als:\n$$ H(X|Y) = -\\sum_{y \\in Y} P(y) \\sum_{x \\in X} P(x|y) \\log_2 P(x|y) $$\n\n## Gegenseitige Information $I(X;Y)$\nDie gegenseitige Information zwischen Wetter und Kleidung wird berechnet als:\n$$ I(X;Y) = H(X) + H(Y) - H(X, Y) $$\n\nDiese Formeln ermöglichen es uns, zu verstehen, wie viel Information das Wetter und die Kleidungswahl gemeinsam haben und wie viel Unsicherheit in einem durch das Wissen des anderen reduziert wird.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Wahrscheinlichkeitsmatrix für Wetter und Kleidung\n\nDie folgende Matrix zeigt die Übergangswahrscheinlichkeiten für die Wahl der Kleidung basierend auf dem aktuellen Wetterzustand:\n\n$$\n\\begin{array}{c|cc}\n\\text{Wetter} & \\text{Mantel} & \\text{T-Shirt} \\\\\n\\hline\n\\text{Sonnig} & P(Coat|Sunny) = 0.1 & P(T-Shirt|Sunny) = 0.9 \\\\\n\\text{Regnerisch} & P(Coat|Rainy) = 0.8 & P(T-Shirt|Rainy) = 0.2 \\\\\n\\end{array}\n$$\noder \n$$\nP(Y|X) = \\begin{bmatrix}\n0.1 & 0.9  \\\\\n0.8 & 0.2  \\\\\n\\end{bmatrix} \n$$\n\n- $P(Coat|Sunny) = 0.1$: Die Wahrscheinlichkeit, dass ein Mantel gewählt wird, wenn das Wetter sonnig ist.\n- $P(T-Shirt|Sunny) = 0.9$: Die Wahrscheinlichkeit, dass ein T-Shirt gewählt wird, wenn das Wetter sonnig ist.\n- $P(Coat|Rainy) = 0.8$: Die Wahrscheinlichkeit, dass ein Mantel gewählt wird, wenn das Wetter regnerisch ist.\n- $P(T-Shirt|Rainy) = 0.2$: Die Wahrscheinlichkeit, dass ein T-Shirt gewählt wird, wenn das Wetter regnerisch ist.\n\nDiese Matrix hilft uns, die bedingte Wahrscheinlichkeit der Kleidungswahl in Abhängigkeit vom Wetter zu verstehen, was für die Berechnung der Entropie und der gegenseitigen Information notwendig ist.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import math\n\n# Define the probabilities\nP_Sunny = 0.7\nP_Rainy = 0.3\n\nP_Coat_Given_Sunny = 0.1\nP_Tshirt_Given_Sunny = 0.9\nP_Coat_Given_Rainy = 0.8\nP_Tshirt_Given_Rainy = 0.2\n\n# Calculate the marginal probabilities of Y (Clothing)\nP_Coat = P_Coat_Given_Sunny * P_Sunny + P_Coat_Given_Rainy * P_Rainy\nP_Tshirt = P_Tshirt_Given_Sunny * P_Sunny + P_Tshirt_Given_Rainy * P_Rainy\n\n# Calculate the entropies\n# Entropy of Weather H(X)\nH_X = -(P_Sunny * math.log2(P_Sunny) + P_Rainy * math.log2(P_Rainy))\n\n# Entropy of Clothing H(Y)\nH_Y = -(P_Coat * math.log2(P_Coat) + P_Tshirt * math.log2(P_Tshirt))\n\n# Joint Entropy H(XY)\nH_XY = -(P_Sunny * P_Tshirt_Given_Sunny * math.log2(P_Sunny * P_Tshirt_Given_Sunny) +\n         P_Sunny * P_Coat_Given_Sunny * math.log2(P_Sunny * P_Coat_Given_Sunny) +\n         P_Rainy * P_Tshirt_Given_Rainy * math.log2(P_Rainy * P_Tshirt_Given_Rainy) +\n         P_Rainy * P_Coat_Given_Rainy * math.log2(P_Rainy * P_Coat_Given_Rainy))\n\n# Conditional Entropy H(Y|X)\nH_Y_given_X = -(P_Sunny * (P_Tshirt_Given_Sunny * math.log2(P_Tshirt_Given_Sunny) +\n                           P_Coat_Given_Sunny * math.log2(P_Coat_Given_Sunny)) +\n                P_Rainy * (P_Tshirt_Given_Rainy * math.log2(P_Tshirt_Given_Rainy) +\n                           P_Coat_Given_Rainy * math.log2(P_Coat_Given_Rainy)))\n\n# Using Bayes' Theorem to calculate these\nP_Sunny_Coat = (P_Coat_Given_Sunny * P_Sunny) / P_Coat\nP_Rainy_Coat = (P_Coat_Given_Rainy * P_Rainy) / P_Coat\nP_Sunny_Tshirt = (P_Tshirt_Given_Sunny * P_Sunny) / P_Tshirt\nP_Rainy_Tshirt = (P_Tshirt_Given_Rainy * P_Rainy) / P_Tshirt\n\n# Now calculate the Conditional Entropy H(X|Y)\nH_X_given_Y = -(P_Coat * (P_Sunny_Coat * math.log2(P_Sunny_Coat) + P_Rainy_Coat * math.log2(P_Rainy_Coat)) +\n                P_Tshirt * (P_Sunny_Tshirt * math.log2(P_Sunny_Tshirt) + P_Rainy_Tshirt * math.log2(P_Rainy_Tshirt)))\n\n# Mutual Information I(X;Y)\nI_X_Y = H_X + H_Y - H_XY\n# or\nI_X_Y_ = H_Y-H_Y_given_X\nI_X_Y__ = H_X-H_X_given_Y\nassert(I_X_Y==I_X_Y_)\nH_X, H_Y, H_XY, H_Y_given_X, H_X_given_Y, I_X_Y\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 35,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(0.8812908992306927,\n 0.8931734583778568,\n 1.4261662432093982,\n 0.5448753439787055,\n 0.5329927848315413,\n 0.34829811439915126)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Zusammenfassung und Schlussfolgerungen\n\nIn diesem Beispiel haben wir die Beziehung zwischen zwei Zufallsvariablen untersucht: dem Wetter (`X`) und der Kleidungswahl (`Y`). Wir haben verschiedene Informations- und Entropiemaße berechnet, um unser Verständnis der Unsicherheit und der Informationsübertragung zwischen diesen Variablen zu vertiefen.\n\n## Berechnete Maße\n\n- **Entropie des Wetters (H(X))**: \\(0.8813\\) Bits\n- **Entropie der Kleidung (H(Y))**: \\(0.8932\\) Bits\n- **Gemeinsame Entropie (H(XY))**: \\(1.4262\\) Bits\n- **Bedingte Entropie der Kleidung gegeben das Wetter (H(Y|X))**: \\(0.5449\\) Bits\n- **Bedingte Entropie des Wetters gegeben die Kleidung (H(X|Y))**: \\(0.533\\) Bits\n- **Synentropie / Transinformation Information (I(X;Y))**: \\(0.3483\\) Bits\n\n## Interpretation\n\nDie Entropie des Wetters und die Entropie der Kleidung waren fast gleich, was darauf hindeutet, dass beide Zufallsvariablen ähnlich unvorhersehbar sind. Die gemeinsame Entropie war größer als die individuellen Entropien, was auf zusätzliche Unsicherheit hindeutet, wenn man beide Variablen gleichzeitig betrachtet.\n\nDie bedingte Entropie von \\(H(Y|X)\\) war niedriger als die von \\(H(X|Y)\\), was zeigt, dass das Wissen über das Wetter die Unsicherheit über die Kleidungswahl stärker reduziert als umgekehrt. Dies ist intuitiv sinnvoll, da das Wetter oft einen direkteren Einfluss auf die Kleidungswahl hat als die Kleidung auf das Wetter.\n\nDie gegenseitige Information von \\(0.3483\\) Bits zeigt, dass zwischen dem Wetter und der Kleidungswahl eine signifikante Informationsüberlappung besteht. Dies quantifiziert den Betrag an Unsicherheit, der in der Kleidungswahl beseitigt wird, wenn das Wetter bekannt ist, und umgekehrt.\n\n## Schlussfolgerung\n\nDieses Beispiel verdeutlicht, wie Entropie und bedingte Entropie dabei helfen können, Entscheidungsfindungen und Unsicherheiten in alltäglichen Situationen zu verstehen. Die gegenseitige Information gibt uns Einblicke, wie viel eine Variable über eine andere aussagt, und ist ein mächtiges Werkzeug, um die Effizienz von Entscheidungsprozessen zu analysieren und zu verbessern.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Fax example\nfrom sympy import Matrix\nimport math\n\n# Create a matrix from the transition probabilities and subtract it from an identity matrix\n# This will create the system of equations (I - P') * π = 0 where π is the steady state distribution vector\n# We need to replace one of the equations with the normalization condition π1 + π2 + π3 = 1\n\nP_w_given_w = 0.85\nP_w_given_b = 0.35\nP_b_given_w = 0.15\nP_b_given_b = 0.65\n\n# Define the transition probability matrix\nP = Matrix([[0.85, 0.15], [0.35, 0.65]])\n\n# Set up the system of equations for the steady state probabilities\n# We create a system (I - P') * π = 0 and replace one row with the normalization condition\nI = Matrix([[1, 0], [0, 1]])\nA = I - P.transpose()\nA[1, :] = Matrix([[1, 1]])  # Replace the last row with the normalization condition\nb = Matrix([0, 1])  # Result matrix for the equations\n\n# Solve the system for the steady state distribution π\nsteady_state_distribution = A.solve_least_squares(b)\nP_w, P_b = steady_state_distribution\n\n# Entropy\nH_X = - P_w * math.log2(P_w) - P_b * math.log2(P_b)\n\n# Conditional Entropy\nH_X_given_Y = - P_w * (P_w_given_w * math.log2(P_w_given_w) + P_b_given_w * math.log2(P_b_given_w))- P_b * (P_w_given_b * math.log2(P_w_given_b)+ P_b_given_b*math.log2(P_b_given_b))\n\n# Synentropy\nH_X_Y = H_X- H_X_given_Y\nprint(f\"Entropy:{H_X}\\tConditional Entropy:{H_X_given_Y}\\tSynentropy:{H_X_Y}\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "text": "Entropy:0.881290899230693\tConditional Entropy:0.707108629914128\tSynentropy:0.174182269316565\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Information Channel example:\n# Define the transition probabilities and the marginal probabilities for X\nP_Y_given_X = [[0.85, 0.15], [0.2, 0.8]]\nP_X = [0.7, 0.3]\n\n# Calculate the marginal probabilities of Y\nP_Y = [0, 0]\nfor i in range(len(P_Y)):\n    for j in range(len(P_X)):\n        P_Y[i] += P_Y_given_X[j][i] * P_X[j]\n\n# Calculate the joint probabilities P(X, Y)\nP_XY = [[0, 0], [0, 0]]\nfor i in range(len(P_X)):\n    for j in range(len(P_Y)):\n        P_XY[i][j] = P_Y_given_X[i][j] * P_X[i]\n\n# Calculate the mutual information I(X; Y)\nI_X_Y = 0\nfor i in range(len(P_X)):\n    for j in range(len(P_Y)):\n        if P_XY[i][j] > 0 and P_Y[j] > 0:\n            I_X_Y += P_XY[i][j] * math.log2(P_XY[i][j] / (P_X[i] * P_Y[j]))\n\nI_X_Y\n\n#Or alternatively\n\nH_x = -0.7 * math.log2(0.7)-0.3*math.log2(0.3)\ny1 = 0.7*0.85+0.3*0.2\ny2 = 0.7* 0.15+0.3*0.8\nH_y = -y1 * math.log2(y1)-y2 * math.log2(y2)\nH_y_given_x = -0.7*(0.85*math.log2(0.85)+0.15*math.log2(0.15))-0.3*(0.20 * math.log2(0.2)+0.8*math.log2(0.8))\nH_x_y = H_y-H_y_given_x\nassert (I_X_Y==H_x_y)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}